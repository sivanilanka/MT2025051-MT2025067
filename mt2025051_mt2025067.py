# -*- coding: utf-8 -*-
"""MT2025051_MT2025067.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qibNQk5PEPk8HgvTEbJSTSt3c9R6_mKA
"""

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

!pip install optuna

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
import optuna

train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')
print("Data loaded successfully.")

test_ids = test_df['id']

train_df = train_df.drop('id', axis=1)
test_df = test_df.drop('id', axis=1)

X = train_df.drop('WeightCategory', axis=1)
y_raw = train_df['WeightCategory']
X_test = test_df

le = LabelEncoder()
y = le.fit_transform(y_raw)
num_classes = len(le.classes_)
print(f"Target variable encoded. Found {num_classes} classes.")

combined_df = pd.concat([X, X_test], axis=0)
combined_processed = pd.get_dummies(combined_df, drop_first=False)

X_processed = combined_processed.iloc[:len(X)]
X_test_processed = combined_processed.iloc[len(X):]
print("Categorical features encoded.")

X_train, X_val, y_train, y_val = train_test_split(
    X_processed,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)
print(f"Training data split for optimization: {len(X_train)} train, {len(X_val)} validation samples.")

# Optuna Function
def objective(trial):
    param = {
        'objective': 'multi:softmax',
        'num_class': num_classes,
        'eval_metric': 'mlogloss',
        'n_estimators': 1000,
        'random_state': 42,
        'n_jobs': -1,
        'early_stopping_rounds': 50,

        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),
    }

    # XGBoost model with the suggested parameters
    xgb_model = xgb.XGBClassifier(**param)

    # Train the model
    xgb_model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=False
    )

    # Store the best number of trees found
    trial.set_user_attr("best_iteration", xgb_model.best_iteration)

    preds = xgb_model.predict(X_val)

    # Return the accuracy
    accuracy = accuracy_score(y_val, preds)
    return accuracy

print("Test 100 different hyperparameter combinations.")
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

print(f"Best validation accuracy: {study.best_value:.4f}")
print("Found best parameters:")
print(study.best_params)

# best parameters
best_params = study.best_params
# optimal number of trees
best_iteration = study.best_trial.user_attrs["best_iteration"]

# Add back the fixed parameters
best_params.update({
    'objective': 'multi:softmax',
    'num_class': num_classes,
    'eval_metric': 'mlogloss',
    'random_state': 42,
    'n_jobs': -1,
    'n_estimators': best_iteration # Use the best number of trees
})

print("\nTraining with the best parameters.")
final_model = xgb.XGBClassifier(**best_params)

# Train on the FULL training dataset
final_model.fit(X_processed, y)

predictions_int = final_model.predict(X_test_processed)
predictions_str = le.inverse_transform(predictions_int)

submission_df = pd.DataFrame({
    'id': test_ids,
    'WeightCategory': predictions_str
})

submission_df.to_csv('submission_optimized.csv', index=False)

from sklearn.metrics import accuracy_score, classification_report

# Predictions on validation set
y_val_pred = final_model.predict(X_val)
val_accuracy = accuracy_score(y_val, y_val_pred)
val_report = classification_report(y_val, y_val_pred)

print(f"Validation Accuracy: {val_accuracy:.4f}")
print("Classification Report:\n", val_report)